---
layout: post
title: "빅데이터 혁신 융합대학 인턴 LLM 개발- NLP 심화 개념(1)"
date: 2024-01-02 09:00:23 +0900
categories:  ["NLP","Intern"]
---
# NLP 심화(1)
<h3>서브워드 토크나이저</h3>
<div class="explain">
서브워드 분리(subword segmentation) 작업은 하나의 단어를 더 작은 단위의 의미있는 여러 서브워드들로 분리해서 단어를 인코딩 및 임베딩한다.
<h4>Byte Pair Encoding</h4>
BPE는 기본적으로 연속적으로 가장 많이 등장한 글자의 쌍을 찾아서 하나의 글자로 병합한다. BPE는 글자 단위에서 점차적으로 단어 집합을 만들어내는 Bottom up 방식의 접근을 사용한다. 우선 훈련 데이터에 있는 단어들을 글자 또는 유니코드 단위로 단위 집합을 만들고 가장 많이 등장하는 유니그램을 하나의 유니그램으로 통합한다.<br>
예를 들어 이 과정을 6번 반복한다면<br>
l o w:5<br>
l o w e r:2<br>
n e w e s t:6<br>
w i d e s t:3<br>가 있으면
<ol>
<li>(e,s)가 가장 빈도 수가 9로 높음 따라서 <br>
l o w:5<br>
l o w e r:2<br>
n e w es t:6<br>
w i d es t:3<br>
</li>
<li>(es,t)가 가장 빈도 수가 9로 높음 따라서 <br>
l o w:5<br>
l o w e r:2<br>
n e w est:6<br>
w i d est:3<br>
</li>
<li>(l,o)가 가장 빈도 수가 7로 높음 따라서 <br>
lo w:5<br>
lo w e r:2<br>
n e w est:6<br>
w i d est:3<br>
</li>
<li>(lo,w)가 가장 빈도 수가 7로 높음 따라서 <br>
low:5<br>
low e r:2<br>
n e w est:6<br>
w i d est:3<br>
</li>
<li>(n,e)가 가장 빈도 수가 6로 높음 따라서 <br>
low:5<br>
low e r:2<br>
ne w est:6<br>
w i d est:3<br>
</li>
<li>(ne,w)가 가장 빈도 수가 6로 높음 따라서 <br>
low:5<br>
low e r:2<br>
new est:6<br>
w i d est:3<br>
</li>
</ol>
이다. 이 결과로 나오는 단어 집합은 l,o,w,e,r,n,s,t,i,d,es,est,lo,low,ne,new이다. 만약 lowset라는 새로운 단어가 들어와도 low과 est의 합성어이기 때문에 이 단어는 OOV가 아니다.
</div>
```python
import re, collections

num_merges = 10

dictionary = {'l o w </w>' : 5,
         'l o w e r </w>' : 2,
         'n e w e s t </w>':6,
         'w i d e s t </w>':3
         }

def get_stats(dictionary):
    pairs = collections.defaultdict(int)
    for word, freq in dictionary.items():
        symbols = word.split()
        for i in range(len(symbols)-1):
            pairs[symbols[i],symbols[i+1]] += freq
    print('현재 pair들의 빈도수 :', dict(pairs))
    return pairs

def merge_dictionary(pair, v_in):
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out

bpe_codes = {}
bpe_codes_reverse = {}

for i in range(num_merges):
    print("### Iteration {}".format(i + 1))
    pairs = get_stats(dictionary)
    best = max(pairs, key=pairs.get)
    dictionary = merge_dictionary(best, dictionary)

    bpe_codes[best] = i
    bpe_codes_reverse[best[0] + best[1]] = best

    print("new merge: {}".format(best))
    print("dictionary: {}".format(dictionary))
```

<div class="explain">
BPE의 변형 알고리즘으로는 WordPiece Tokenizer와 Unigram Language Model Tokenizer가 있다. WordPiece Tokenizer는 BPE가 가장 빈도가 높은 쌍을 병합하는 것과 달리 병합되었을떄 코퍼스 likelihood가 가장 높은 쌍을 병합한다. Unigram Language Model Tokenizer는 각각의 서브워드들의 서브워드가 제거되었을 때 코퍼스 likelihood가 감소하는 정도인 손실을 계산한다. 그리고 최악의 영향을 주는 10%~20%의 토큰을 제거한다. 
<h4>센텐스피스</h4>
사전 토큰화 작업(pretokenization)없이 전처리를 하지 않은 데이터(raw data)에 바로 단어 분리 토크나이저를 사용할 수 있다면, 이 토크나이저는 그 어떤 언어에도 적용할 수 있는 토크나이저가 될 것이다. 센텐스피스(SentencePiece)는 이러한 이점을 살려서 구현되었다. 센텐스피스는 사전 토큰화 작업없이 단어 분리 토큰화를 수행하므로 언어에 종속되지 않는다.
</div>
```console
> pip install sentencepiece
```
```python
import sentencepiece as spm
import pandas as pd
import urllib.request
import csv

urllib.request.urlretrieve("https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv", filename="IMDb_Reviews.csv")

train_df = pd.read_csv('IMDb_Reviews.csv')

with open('imdb_review.txt', 'w', encoding='utf8') as f:
    f.write('\n'.join(train_df['review']))

spm.SentencePieceTrainer.Train('--input=imdb_review.txt --model_prefix=imdb --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')

vocab_list = pd.read_csv('imdb.vocab', sep='\t', header=None, quoting=csv.QUOTE_NONE)
vocab_list.sample(10)

sp = spm.SentencePieceProcessor()
vocab_file = "imdb.model"

lines = [
  "I didn't at all think of it this way.",
  "I have waited a long time for someone to film"
]
for line in lines:
  print(line)
  print(sp.encode_as_pieces(line))
  print(sp.encode_as_ids(line))
  print()
```

<div class="explain">
<h4>서브워드텍스트인코더</h4>
SubwordTextEncoder는 텐서플로우를 통해 사용할 수 있으며 WordPiece Model을 사용한다.
</div>
```python
import pandas as pd
import urllib.request
import tensorflow_datasets as tfds

urllib.request.urlretrieve("https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv", filename="IMDb_Reviews.csv")

train_df = pd.read_csv('IMDb_Reviews.csv')

tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(
    train_df['review'], target_vocab_size=2**13)

sample_string = "It's mind-blowing to me that this film was even made."

tokenized_string = tokenizer.encode(sample_string)
print ('정수 인코딩 후의 문장 : {}'.format(tokenized_string))

original_string = tokenizer.decode(tokenized_string)
print ('기존 문장 : {}'.format(original_string))
```
<div class="explain">
<h4>허깅페이스 인코딩</h4>
Huggingface에서 여러 토큰나이저를 제공한다.
</div>
```console
> pip install tokenizers
```
```python
import urllib.request
from tokenizers import BertWordPieceTokenizer

urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt", filename="ratings.txt")

naver_df = pd.read_table('ratings.txt')
naver_df = naver_df.dropna(how='any')
with open('naver_review.txt', 'w', encoding='utf8') as f:
    f.write('\n'.join(naver_df['document']))

tokenizer = BertWordPieceTokenizer(lowercase=False, trip_accents=False)

data_file = 'naver_review.txt'
vocab_size = 30000
limit_alphabet = 6000
min_frequency = 5

tokenizer.train(files=data_file,
                vocab_size=vocab_size,
                limit_alphabet=limit_alphabet,
                min_frequency=min_frequency)

tokenizer.save_model('./')

encoded = tokenizer.encode('아 배고픈데 짜장면먹고싶다')
print('토큰화 결과 :',encoded.tokens)
print('정수 인코딩 :',encoded.ids)
print('디코딩 :',tokenizer.decode(encoded.ids))
```
```python
from tokenizers import ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer

tokenizer = SentencePieceBPETokenizer()
tokenizer.train('naver_review.txt', vocab_size=10000, min_frequency=5)

encoded = tokenizer.encode("이 영화는 정말 재미있습니다.")
print(encoded.tokens)
```
<h3>RNN 인코더-디코더</h3>
<div class="explain">
인코더-디코더 구조는 주로 입력 문장과 출력 문장의 길이가 다를 때 사용한다. 이 구조는 하나의 RNN을 인코더, 다른 하나의 RNN을 디코더로 사용한다.
<h4>시퀀스-투-시퀀스(seq2seq)</h4>
seq2seq는 번역기에서 대표적으로 사용하는 모델이다. seq2seq에서 인코더는 입력 문장의 모든 단어들을 순차적으로 입력받은 뒤 마지막에 모든 단어 정보들을 합축해서 하나의 벡터로 만드는데 이를 컨텍스트 벡터(context vector)라고 한다. 입력 문장의 정보가 하나의 컨텍스트 벡터로 모두 압축되면 인코더는 컨텍스트 벡터를 디코더로 전송한다. 디코더는 컨텍스트 벡터를 받아서 번역된 단어를 한 개씩 순차적으로 출력한다. 인코딩에서 입력 문장은 단어 토큰화를 통해서 단어 단위로 쪼개지고 단어 토큰 각각은 RNN 셀의 각 시점의 입력된다. 그리고 모든 단어를 입력 받은 뒤 마지막 시점에서 은닉상태를 디토더 RNN으로 넘겨주는데 이는 디코더 RNN의 첫번째 은닉상태에 사용된다. 디코더 RNN에서는 초기 입력으로 문장의 시작을 의미하는 &lt;sos&gt;가 들어간다. 각 시점마다 RNN은 다음으로 등장할 확률이 높은 단어를 예측하고 다음 시점의 입력으로 보낸다. 이를 문장의 끝을 의미하는 심볼인 &lt;eos&gt;가 예측될때까지 반복한다. 
</div>
```python
import os
import shutil
import zipfile

import pandas as pd
import tensorflow as tf
import urllib3
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

import requests
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense
from tensorflow.keras.models import Model
import numpy as np

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def download_zip(url, output_path):
    response = requests.get(url, headers=headers, stream=True)
    if response.status_code == 200:
        with open(output_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        print(f"ZIP file downloaded to {output_path}")
    else:
        print(f"Failed to download. HTTP Response Code: {response.status_code}")

url = "http://www.manythings.org/anki/fra-eng.zip"
output_path = "fra-eng.zip"
download_zip(url, output_path)

path = os.getcwd()
zipfilename = os.path.join(path, output_path)

with zipfile.ZipFile(zipfilename, 'r') as zip_ref:
    zip_ref.extractall(path)

lines = pd.read_csv('fra.txt', names=['src', 'tar', 'lic'], sep='\t')

lines = lines.loc[:, 'src':'tar']
lines = lines[0:60000] 
lines.sample(10)

lines.tar = lines.tar.apply(lambda x : '\t '+ x + ' \n')

src_vocab = set()
for line in lines.src: 
    for char in line: 
        src_vocab.add(char)

tar_vocab = set()
for line in lines.tar:
    for char in line:
        tar_vocab.add(char)

src_vocab = sorted(list(src_vocab))
tar_vocab = sorted(list(tar_vocab))

encoder_input = []

for line in lines.src:
  encoded_line = []
  for char in line:
    encoded_line.append(src_to_index[char])
  encoder_input.append(encoded_line)

decoder_input = []
for line in lines.tar:
  encoded_line = []
  for char in line:
    encoded_line.append(tar_to_index[char])
  decoder_input.append(encoded_line)

decoder_target = []
for line in lines.tar:
  timestep = 0
  encoded_line = []
  for char in line:
    if timestep > 0:
      encoded_line.append(tar_to_index[char])
    timestep = timestep + 1
  decoder_target.append(encoded_line)

max_src_len = max([len(line) for line in lines.src])
max_tar_len = max([len(line) for line in lines.tar])

encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')
decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')
decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')

encoder_input = to_categorical(encoder_input)
decoder_input = to_categorical(decoder_input)
decoder_target = to_categorical(decoder_target)

encoder_inputs = Input(shape=(None, src_vocab_size))
encoder_lstm = LSTM(units=256, return_state=True)

encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)

encoder_states = [state_h, state_c]

decoder_inputs = Input(shape=(None, tar_vocab_size))
decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)

decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states)

decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')
decoder_outputs = decoder_softmax_layer(decoder_outputs)

model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer="rmsprop", loss="categorical_crossentropy")

model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=40, validation_split=0.2)

encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)

decoder_state_input_h = Input(shape=(256,))
decoder_state_input_c = Input(shape=(256,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)

decoder_states = [state_h, state_c]
decoder_outputs = decoder_softmax_layer(decoder_outputs)
decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)

index_to_src = dict((i, char) for char, i in src_to_index.items())
index_to_tar = dict((i, char) for char, i in tar_to_index.items())

def decode_sequence(input_seq):
  states_value = encoder_model.predict(input_seq)

  target_seq = np.zeros((1, 1, tar_vocab_size))
  target_seq[0, 0, tar_to_index['\t']] = 1.

  stop_condition = False
  decoded_sentence = ""

  while not stop_condition:
    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)
    sampled_token_index = np.argmax(output_tokens[0, -1, :])
    sampled_char = index_to_tar[sampled_token_index]
    decoded_sentence += sampled_char
    if (sampled_char == '\n' or
        len(decoded_sentence) > max_tar_len):
        stop_condition = True
    target_seq = np.zeros((1, 1, tar_vocab_size))
    target_seq[0, 0, sampled_token_index] = 1.
    states_value = [h, c]

  return decoded_sentence

for seq_index in [3,50,100,300,1001]: 
  input_seq = encoder_input[seq_index:seq_index+1]
  decoded_sentence = decode_sequence(input_seq)
  print(35 * "-")
  print('입력 문장:', lines.src[seq_index])
  print('정답 문장:', lines.tar[seq_index][2:len(lines.tar[seq_index])-1]) 
  print('번역 문장:', decoded_sentence[1:len(decoded_sentence)-1]) 
```
<div class="explain">
<h4>BLEU  Score</h4>
BLEU(bilingual Evaluation Understudy)는 기계 번역 결과와 사람이 직접 번역한 결과가 얼마나 유사한지 비교하여 번역에 대한 성능을 측정하는 방법이다. 측정 기준은 n-gram에 기반한다.
</div>
```python
import nltk.translate.bleu_score as bleu

candidate = 'It is a guide to action which ensures that the military always obeys the commands of the party'
references = [
    'It is a guide to action that ensures that the military will forever heed Party commands',
    'It is the guiding principle which guarantees the military forces always being under the command of the Party',
    'It is the practical guide for the army always to heed the directions of the party'
]

print('실습 코드의 BLEU :',bleu_score(candidate.split(),list(map(lambda ref: ref.split(), references))))
print('패키지 NLTK의 BLEU :',bleu.sentence_bleu(list(map(lambda ref: ref.split(), references)),candidate.split()))
```
<h3>출처</h3>
<a href="https://wikidocs.net/book/2155">딥러닝을 이용한 자연어 처리 입문</a>
