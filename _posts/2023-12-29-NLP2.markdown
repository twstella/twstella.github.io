---
layout: post
title: "빅데이터 혁신 융합대학 인턴 LLM 개발- NLP 기본 개념(2)"
date: 2023-12-30 09:00:23 +0900
categories:  ["NLP","Intern"]
---
# NLP 기본 개념(2)
<h3>단어 표현 방법</h3>

<div class="explain">
<h4>Bag of Words</h4>
단어의 순서는 전혀 고려하지 않고 단어의 출현 빈도에만 집중하는 텍스트 데이터의 수치화 표현 방법이다.<br>
sklearn(사이킷런)에서는 CountVectorizer 클래스를 지원한다. 
</div>
```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = ['If I were a bird, I would fly to you.']
vector = CountVectorizer()
print( vector.fit_transform(corpus).toarray()) 
print(vector.vocabulary_)
```

<div class="explain">
<h4>DTM</h4>
문서 단어 행렬(Document-Term Matrix)은 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 자타낸 것이다. 그러나 원-핫 벡터와 마찬가지로 대부분의 값이 0인 희소행렬이라는 문제가 있다.
</div>

<div class="explain">
<h4>TF-IDF</h4>
TF-IDF(Term Frequency-Inverse Document Frequency)는 단어의 빈도와 역 문서 빈도를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법이다. TF-IDF는 TF와 IDF를 곱한 값을 의미하는데 문서를 d,단어를 t, 문서의 총 개수를 n이라고 할때 TF, DF, IDF는 다음과 같이 정의할 수 있다.
</div>
<ul>
<li>tf(d,t): 특정 문서 d에서의 특정 단어 t의 등장 횟수</li>
<li>df(t): 특정 단어 t가 등장한 문서의 수</li>
<li>idf(t): df(t)에 반비례하는 수로 <span class="math">log(n/(1+df(t)))</span></li>
</ul>
<div class="explain">
TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며 특정 문서에서만 자주 등장하는 단어의 중요도가 높다고 판단한다. TF-IDF 값이 낮으면 중요도가 낮으며 TF-IDF 값이 크면 중요도가 높다. sklearn은 TF-IDF를 자동 계산해주는 TfidfVectorizer를 제공한다.
</div>
```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    'If I were a bird I would fly to you',
    'Rose are red violets are blue',
    'Mary had a little lamb little lamb',    
]

vector = CountVectorizer()
print(vector.fit_transform(corpus).toarray())
print(vector.vocabulary_)

tfidfv = TfidfVectorizer().fit(corpus)
print(tfidfv.transform(corpus).toarray())
print(tfidfv.vocabulary_)
```
<h3>벡터 유사도</h3>

<div class="explain">
<h4>코사인 유사도</h4>
코사인 유사도는 두 벡터간의 코사인 각도를 이용하여 구할 수 있는 두 벡터의 유사도로 두 벡터의 값이 동일한 경우에는 1, 90도 일 때는 0, 180도로 반대 방향을 가지면 -1의 값을 가진다.<br>
TF-IDF와 코사인 유사도만을 가지고도 추천 시스템을 만들 수 있다. 
</div>
```python
import numpy as np
from numpy import dot
from numpy.linalg import norm

def cos_sim(A, B):
  return dot(A, B)/(norm(A)*norm(B))

doc1 = np.array([0,1,1,1])
doc2 = np.array([1,0,1,1])
doc3 = np.array([2,0,2,2])

print(cos_sim(doc1, doc2))
print(cos_sim(doc1, doc3))
print(cos_sim(doc2, doc3))
```

<div class="explain">
<h4>유클리드 거리</h4>
벡터 사이의 거리를 구하는 방법으로 <span class="math">A=(a1,a2,a3,..)</span>이고  <span class="math">B=(b1,b2,b3,..)</span>일때 유클리드 거리는 <span class="math">sqrt((pow(a1-b1,2)+pow(a2,b2),2)+pow(a3-b3,2)+...,2)</span>이다.
</div>
```python
import numpy as np

def dist(x,y):   
    return np.sqrt(np.sum((x-y)**2))

doc1 = np.array((2,3,0,1))
doc2 = np.array((1,2,3,1))
doc3 = np.array((2,1,2,2))
docQ = np.array((1,1,0,1))

print(dist(doc1,docQ))
print(dist(doc2,docQ))
print(dist(doc3,docQ))
```

<div class="explain">
<h4>자카드 유사도</h4>
합집합에서 교집합의 비율을 구한다. 0과 1 사이의 값을 가지며 동일할 수록 1에 가까운 값을 가진다. 자카드 유사도는 두 집합의 교집합의 크기를 두 집합의 합집합으로 나눈 값이다.
</div>

<h3>워드 임베딩</h3>
<div class="explain">
희소 표현은 원-핫 인코딩을 통해서 워드를 임베딩하는 방법으로 단어의 인덱스만 1로 표현하고 나머지는 0으로 표현하는 방법이다. <br>
희소 표현과 반대되는 표현으로 밀집 표현은 벡저의 차원을 사용자가 설정한 값으로 모든 단어의 벡터 차원을 맞춘다. 워드 임베딩(Word Embedding)은 단어를 밀접 벡터 형태로 표현하는 방법이고 임베딩 과정으로 통해 나온 벡터를 임베딩 벡터라고 한다. 워드 임베딩 방법으로는 LSA, Word2Vec, FastText, Glove 등이 있다. 
<br>
단어의 의미를 다차원 공간에 벡터화하는 방법을 분산 표현(distributed representation)이라고 한다. 분산 표현은 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다는 가정하에 만들어졌다. 이러한 가정을 분포 가설이라고 한다. 분산 표현은 분포 가설을 이용하여 텍스트를 학습하고, 단어의 의미를 벡터의 여러 차원에 분산하여 표현한다.
분산 표현은 저차원에서 단어의 의미를 여러 차원에다가 분산하여 표현하여 단어 벡터 간의 유의미한 유사도를 계산할 수 있다. 이를 위한 대표젹인 학습 방법은 Word2Vec이며 이 학습 방법에는 CBOW와 Skip-Gram이 있다. CBOW는 주변에 있는 단어들을 입력으로 중간에 있는 단어를 예측하는 방법이고 Skip-gram은 반대로 중간에 있는 단어들을 입력으로 주변 단어들을 예측하는 방법이다.<br>
NNLM은 예측 단어 이전의 단어만을 참고하지만 Word2Vec는 예측 단어의 전,후 단어들을 모두 참고한다. 
<h4>CBOW(Continuous Bag of Words)</h4>
예측해야하는 단어를 중심 단어(center word)라고 하고 예측에 사용되는 단어들을 주변 단어(context word)라고 한다. 중심 단어를 예측하기 위해서 앞, 뒤로 몇 개의 단어를 볼지를 정해야하는 데 이 범위를 윈도우(window)라고 한다. 윈도우의 크기가 n이라고 하면 중심 단어를 예측하기 위해 참고하려는 하는 주변 단어의 개수는 2n개 이다. 윈도우의 크기가 정해지면 윈도우를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가며 학습을 위한 데이터 셋을 만드는데 이 방법을 슬라이딩 윈도우(sliding window)라고 한다. <br>
CBOW 인공 신경망은 입력층(input layer)으로는 사용자가 정한 윈도우 크기의 앞, 뒤의 주변 단어들의 원-핫 벡터가 들어가고 출력층(Output layer)으로 예측하고자하는 중간 단어의 원-핫 벡터가 레이블로 주어진다. 이 안공 신경망은 은닉층의 1개인 얕은 신경망이며 일반적인 딥러닝 신경망과는 달리 활성화 함수가 존재하지 않고 룩업 테이블(lookup table)이라는 연산을 하는 투사층(projection layer)가 있다. 
</div>
<img   src='{{ "public/img/Weight1.jpg" | relative_url }}' alt='relative'/>
<img   src='{{ "public/img/Weight2.jpg" | relative_url }}' alt='relative'/>
<img   src='{{ "public/img/Weight3.jpg" | relative_url }}' alt='relative'/>
<div class="explain">
입력층과 투사층의 사이의 가중치 W는 VxM 행렬이며, 투사층에서 출력층 사이의 가중치 W'는 MxV 행렬이고 인공 신경망의 훈련 전에 행렬 W와 W'는 렌덤 값을 가지고 있다. 
원-핫 벡터와 가중치 행렬을 곱하는 연산을 룩업 테이블(lookup table)이라고 하는데 그 이유는 원-핫 벡터와 가중치의 곱은 가중치 행렬의 i번째 행을 그대로 읽어오는 것과 같기 때문이다. lookup 한 W의 각 행 벡터는 Word2Vec 학습 후에는 각 단어의 M차원의 임베딩 벡터로 간주된다. 이렇게 주변 단어의 원-핫 벡터에 대해서 가중치 W를 곱해서 생겨진 결과 벡터들을 투사층에서 만나서 벡터들의 평균을 구하는데 사용된다. 이 부분이 Skip-gram과의 차이점이다. Skip-gram은 입력이 중심 단어이기 떄문에 투사층에서 벡터의 평균을 구하지 않는다. 이렇게 구해진 평균 벡터는 두번쨰 가중치 행렬 W'과 곱해진다. 곱셈의 결과로는 원-핫 백터들과 차원이 V로 같은 벡터가 나온다. 이 벡터가 softmax함수를 지나면 각 원소들은 0과 1사이의 값이고 총 합은 1인 벅터가 된다. 이는 다중 클래스 분류 문제를 위한 일종의 스코어 벡터(score vector)이다. 이 벡터와 이전에 출력 레이블로 준 벡터와의 오차를 줄이기 위해 크로스 엔트로피를 구하고 이를 역전파를 하며 W와 W'을 학습한다.
<h4>Skip-gram</h4>
CBOW에서는 주변 단어를 통해 중심 단어를 예측했다면, Skip-gram은 중심 단어에서 주변 단어를 예측한다. 중심 단어에 대해서 주변 단어를 예측하므로 투사층에서 벡터들의 평균을 구하는 과정은 없다. 여러 논문에서 성능 비교를 진행했을 때 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려져 있다.<br>
<img   src='{{ "public/img/Weight4.jpg" | relative_url }}' alt='relative'/>
<h4>Negative Sampling</h4>
Word2Vec의 출력층에서는 소프트맥스 함수를 지난 단어 집합의 크기의 벡터와 실제값인 원-핫 벡터와의 오차를 구하고 이를 기반으로 임베딩 테이블에 있는 모든 단어에 대한 임베딩 벡터 값을 업데이트한다.그러나 만약 단어의 집합의 크기가 수만 이상에 달하면 이 작업은 굉장히 무거운 작업이 된다. 네거티브 샘플링(Nagative Sampling)은 Word2Vec가 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 하는 방법이다. 주변 단어들을 긍정, 랜덤으로 샘플링된 단어들을 부정으로 레이블링하면 이진 분류 문제를 위한 데이터셋이 된다. Skip-gram은 기본적으로 중심 단어를 입력, 주변 단어를 레이블로 한다. SGNS(Skip-gram with Negative Sampling)을 학습하기 위해서는 기존의 Skip-gram 데이터셋에서 중심 단어와 주변 단어를 각각 입력1, 입력2로 두고 실제 윈도우 크기 내에서 이웃 관계였으면 레이블을 1로 준다. 입력1(중심단어)과 주변 단어 관계가 아닌 단어들을 입력2로 삼기 위해서는 단어 집합에서 랜덤으로 단어들을 선택하고 레이블을 0으로 준다. 그리고 입력1인 중심 단어의 테이블 룩업을 위한 임베딩 테이블과 입력 2인 주변 단어의 테이블 룩업을 위한 임베딩 테이블을 준비한다. 각 단어는 임베딩 테이블을 테이블 룩업하여 임베딩 벡터로 변환한다. 그 후 중심 단어와 주변 단어의 내적값을 모델의 예측값으로 하고 레이블과의 오차를 역전파하여 중심 단어와 주변 단어의 임베딩 벡터 값을 업데이트한다. 
<h4>GloVe</h4>
LSA는 카운트 기반으로 코퍼스의 전체적인 통계 정보를 고려하기는 하지만 단어 의미 유추 작업(Analogy task)에는 성능이 떨어지고 Word2Vec은 예측 기반으로 단어 간 유추 작업에는 LSA보다 뛰어나지만 임베딩 벡터가 원도우 크기 내에서만 주변 단어를 고려하기 때문에 코퍼스의 전체적인 통계 정보를 반영하지 못한다. GloVe는 LSA의 메커니즘이었던 카운트 기반의 방법과 Word2Vec의 메커니즘이었던 예측 기반의 방법론 모두를 사용한다. 윈도우 기반 동시 등장 행랼(Window based Co-occurence Matrix)는 행과 열을 전체 단어 집합의 단어들로 구성하고 i단어의 윈도우 크기 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬이다. 동시 등장 확률(Co-occurrence Probability)는 <span class="math">P(k|i)</span>로 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고 특정 단어가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률이다.<br>

</div>
<h3>출처</h3>
<a href="https://wikidocs.net/book/2155">딥러닝을 이용한 자연어 처리 입문</a>
