---
layout: post
title: "빅데이터 혁신 융합대학 인턴 LLM 개발- NLP 기본 개념(2)"
date: 2023-12-30 09:00:23 +0900
categories:  ["NLP","Intern"]

---
# NLP 기본 개념(2)
<h3>단어 표현 방법</h3>

<div class="explain">
<h4>Bag of Words</h4>
단어의 순서는 전혀 고려하지 않고 단어의 출현 빈도에만 집중하는 텍스트 데이터의 수치화 표현 방법이다.<br>
sklearn(사이킷런)에서는 CountVectorizer 클래스를 지원한다. 
</div>
```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = ['If I were a bird, I would fly to you.']
vector = CountVectorizer()
print( vector.fit_transform(corpus).toarray()) 
print(vector.vocabulary_)
```

<div class="explain">
<h4>DTM</h4>
문서 단어 행렬(Document-Term Matrix)은 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 자타낸 것이다. 그러나 원-핫 벡터와 마찬가지로 대부분의 값이 0인 희소행렬이라는 문제가 있다.
</div>

<div class="explain">
<h4>TF-IDF</h4>
TF-IDF(Term Frequency-Inverse Document Frequency)는 단어의 빈도와 역 문서 빈도를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법이다. TF-IDF는 TF와 IDF를 곱한 값을 의미하는데 문서를 d,단어를 t, 문서의 총 개수를 n이라고 할때 TF, DF, IDF는 다음과 같이 정의할 수 있다.
</div>
<ul>
<li>tf(d,t): 특정 문서 d에서의 특정 단어 t의 등장 횟수</li>
<li>df(t): 특정 단어 t가 등장한 문서의 수</li>
<li>idf(t): df(t)에 반비례하는 수로 <span class="math">log(n/(1+df(t)))</span></li>
</ul>
<div class="explain">
TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며 특정 문서에서만 자주 등장하는 단어의 중요도가 높다고 판단한다. TF-IDF 값이 낮으면 중요도가 낮으며 TF-IDF 값이 크면 중요도가 높다. sklearn은 TF-IDF를 자동 계산해주는 TfidfVectorizer를 제공한다.
</div>
```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    'If I were a bird I would fly to you',
    'Rose are red violets are blue',
    'Mary had a little lamb little lamb',    
]

vector = CountVectorizer()
print(vector.fit_transform(corpus).toarray())
print(vector.vocabulary_)

tfidfv = TfidfVectorizer().fit(corpus)
print(tfidfv.transform(corpus).toarray())
print(tfidfv.vocabulary_)
```
<h3>벡터 유사도</h3>

<div class="explain">
<h4>코사인 유사도</h4>
코사인 유사도는 두 벡터간의 코사인 각도를 이용하여 구할 수 있는 두 벡터의 유사도로 두 벡터의 값이 동일한 경우에는 1, 90도 일 때는 0, 180도로 반대 방향을 가지면 -1의 값을 가진다.<br>
TF-IDF와 코사인 유사도만을 가지고도 추천 시스템을 만들 수 있다. 
</div>
```python
import numpy as np
from numpy import dot
from numpy.linalg import norm

def cos_sim(A, B):
  return dot(A, B)/(norm(A)*norm(B))

doc1 = np.array([0,1,1,1])
doc2 = np.array([1,0,1,1])
doc3 = np.array([2,0,2,2])

print(cos_sim(doc1, doc2))
print(cos_sim(doc1, doc3))
print(cos_sim(doc2, doc3))
```

<div class="explain">
<h4>유클리드 거리</h4>
벡터 사이의 거리를 구하는 방법으로 <span class="math">A=(a1,a2,a3,..)</span>이고  <span class="math">B=(b1,b2,b3,..)</span>일때 유클리드 거리는 <span class="math">sqrt((pow(a1-b1,2)+pow(a2,b2),2)+pow(a3-b3,2)+...,2)</span>이다.
</div>
```python
import numpy as np

def dist(x,y):   
    return np.sqrt(np.sum((x-y)**2))

doc1 = np.array((2,3,0,1))
doc2 = np.array((1,2,3,1))
doc3 = np.array((2,1,2,2))
docQ = np.array((1,1,0,1))

print(dist(doc1,docQ))
print(dist(doc2,docQ))
print(dist(doc3,docQ))
```

<div class="explain">
<h4>자카드 유사도</h4>
합집합에서 교집합의 비율을 구한다. 0과 1 사이의 값을 가지며 동일할 수록 1에 가까운 값을 가진다. 자카드 유사도는 두 집합의 교집합의 크기를 두 집합의 합집합으로 나눈 값이다.
</div>

<h3>워드 임베딩</h3>
<div class="explain">
희소 표현은 원-핫 인코딩을 통해서 워드를 임베딩하는 방법으로 단어의 인덱스만 1로 표현하고 나머지는 0으로 표현하는 방법이다. <br>
희소 표현과 반대되는 표현으로 밀집 표현은 벡저의 차원을 사용자가 설정한 값으로 모든 단어의 벡터 차원을 맞춘다. 워드 임베딩(Word Embedding)은 단어를 밀접 벡터 형태로 표현하는 방법이고 임베딩 과정으로 통해 나온 벡터를 임베딩 벡터라고 한다. 워드 임베딩 방법으로는 LSA, Word2Vec, FastText, Glove 등이 있다. 
<br>
단어의 의미를 다차원 공간에 벡터화하는 방법을 분산 표현(distributed representation)이라고 한다. 분산 표현은 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다는 가정하에 만들어졌다. 이러한 가정을 분포 가설이라고 한다. 분산 표현은 분포 가설을 이용하여 텍스트를 학습하고, 단어의 의미를 벡터의 여러 차원에 분산하여 표현한다.
분산 표현은 저차원에서 단어의 의미를 여러 차원에다가 분산하여 표현하여 단어 벡터 간의 유의미한 유사도를 계산할 수 있다. 이를 위한 대표젹인 학습 방법은 Word2Vec이며 이 학습 방법에는 CBOW와 Skip-Gram이 있다. CBOW는 주변에 있는 단어들을 입력으로 중간에 있는 단어를 예측하는 방법이고 Skip-gram은 반대로 중간에 있는 단어들을 입력으로 주변 단어들을 예측하는 방법이다.<br>
NNLM은 예측 단어 이전의 단어만을 참고하지만 Word2Vec는 예측 단어의 전,후 단어들을 모두 참고한다. 
<h4>CBOW(Continuous Bag of Words)</h4>
예측해야하는 단어를 중심 단어(center word)라고 하고 예측에 사용되는 단어들을 주변 단어(context word)라고 한다. 중심 단어를 예측하기 위해서 앞, 뒤로 몇 개의 단어를 볼지를 정해야하는 데 이 범위를 윈도우(window)라고 한다. 윈도우의 크기가 n이라고 하면 중심 단어를 예측하기 위해 참고하려는 하는 주변 단어의 개수는 2n개 이다. 윈도우의 크기가 정해지면 윈도우를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가며 학습을 위한 데이터 셋을 만드는데 이 방법을 슬라이딩 윈도우(sliding window)라고 한다. <br>
CBOW 인공 신경망은 입력층(input layer)으로는 사용자가 정한 윈도우 크기의 앞, 뒤의 주변 단어들의 원-핫 벡터가 들어가고 출력층(Output layer)으로 예측하고자하는 중간 단어의 원-핫 벡터가 레이블로 주어진다. 이 안공 신경망은 은닉층의 1개인 얕은 신경망이며 일반적인 딥러닝 신경망과는 달리 활성화 함수가 존재하지 않고 룩업 테이블(lookup table)이라는 연산을 하는 투사층(projection layer)가 있다. 
</div>
<img   src='{{ "public/img/Weight1.jpg" | relative_url }}' alt='relative'/>
<img   src='{{ "public/img/Weight2.jpg" | relative_url }}' alt='relative'/>
<img   src='{{ "public/img/Weight3.jpg" | relative_url }}' alt='relative'/>
<div class="explain">
입력층과 투사층의 사이의 가중치 W는 VxM 행렬이며, 투사층에서 출력층 사이의 가중치 W'는 MxV 행렬이고 인공 신경망의 훈련 전에 행렬 W와 W'는 렌덤 값을 가지고 있다. 
원-핫 벡터와 가중치 행렬을 곱하는 연산을 룩업 테이블(lookup table)이라고 하는데 그 이유는 원-핫 벡터와 가중치의 곱은 가중치 행렬의 i번째 행을 그대로 읽어오는 것과 같기 때문이다. lookup 한 W의 각 행 벡터는 Word2Vec 학습 후에는 각 단어의 M차원의 임베딩 벡터로 간주된다. 이렇게 주변 단어의 원-핫 벡터에 대해서 가중치 W를 곱해서 생겨진 결과 벡터들을 투사층에서 만나서 벡터들의 평균을 구하는데 사용된다. 이 부분이 Skip-gram과의 차이점이다. Skip-gram은 입력이 중심 단어이기 떄문에 투사층에서 벡터의 평균을 구하지 않는다. 이렇게 구해진 평균 벡터는 두번쨰 가중치 행렬 W'과 곱해진다. 곱셈의 결과로는 원-핫 백터들과 차원이 V로 같은 벡터가 나온다. 이 벡터가 softmax함수를 지나면 각 원소들은 0과 1사이의 값이고 총 합은 1인 벅터가 된다. 이는 다중 클래스 분류 문제를 위한 일종의 스코어 벡터(score vector)이다. 이 벡터와 이전에 출력 레이블로 준 벡터와의 오차를 줄이기 위해 크로스 엔트로피를 구하고 이를 역전파를 하며 W와 W'을 학습한다.
<h4>Skip-gram</h4>
CBOW에서는 주변 단어를 통해 중심 단어를 예측했다면, Skip-gram은 중심 단어에서 주변 단어를 예측한다. 중심 단어에 대해서 주변 단어를 예측하므로 투사층에서 벡터들의 평균을 구하는 과정은 없다. 여러 논문에서 성능 비교를 진행했을 때 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려져 있다.<br>
<img   src='{{ "public/img/Weight4.jpg" | relative_url }}' alt='relative'/>
<h4>Negative Sampling</h4>
Word2Vec의 출력층에서는 소프트맥스 함수를 지난 단어 집합의 크기의 벡터와 실제값인 원-핫 벡터와의 오차를 구하고 이를 기반으로 임베딩 테이블에 있는 모든 단어에 대한 임베딩 벡터 값을 업데이트한다.그러나 만약 단어의 집합의 크기가 수만 이상에 달하면 이 작업은 굉장히 무거운 작업이 된다. 네거티브 샘플링(Nagative Sampling)은 Word2Vec가 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 하는 방법이다. 주변 단어들을 긍정, 랜덤으로 샘플링된 단어들을 부정으로 레이블링하면 이진 분류 문제를 위한 데이터셋이 된다. Skip-gram은 기본적으로 중심 단어를 입력, 주변 단어를 레이블로 한다. SGNS(Skip-gram with Negative Sampling)을 학습하기 위해서는 기존의 Skip-gram 데이터셋에서 중심 단어와 주변 단어를 각각 입력1, 입력2로 두고 실제 윈도우 크기 내에서 이웃 관계였으면 레이블을 1로 준다. 입력1(중심단어)과 주변 단어 관계가 아닌 단어들을 입력2로 삼기 위해서는 단어 집합에서 랜덤으로 단어들을 선택하고 레이블을 0으로 준다. 그리고 입력1인 중심 단어의 테이블 룩업을 위한 임베딩 테이블과 입력 2인 주변 단어의 테이블 룩업을 위한 임베딩 테이블을 준비한다. 각 단어는 임베딩 테이블을 테이블 룩업하여 임베딩 벡터로 변환한다. 그 후 중심 단어와 주변 단어의 내적값을 모델의 예측값으로 하고 레이블과의 오차를 역전파하여 중심 단어와 주변 단어의 임베딩 벡터 값을 업데이트한다. 
<h4>GloVe</h4>
LSA는 카운트 기반으로 코퍼스의 전체적인 통계 정보를 고려하기는 하지만 단어 의미 유추 작업(Analogy task)에는 성능이 떨어지고 Word2Vec은 예측 기반으로 단어 간 유추 작업에는 LSA보다 뛰어나지만 임베딩 벡터가 원도우 크기 내에서만 주변 단어를 고려하기 때문에 코퍼스의 전체적인 통계 정보를 반영하지 못한다. GloVe는 LSA의 메커니즘이었던 카운트 기반의 방법과 Word2Vec의 메커니즘이었던 예측 기반의 방법론 모두를 사용한다. 윈도우 기반 동시 등장 행랼(Window based Co-occurence Matrix)는 행과 열을 전체 단어 집합의 단어들로 구성하고 i단어의 윈도우 크기 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬이다. 동시 등장 확률(Co-occurrence Probability)는 <span class="math">P(k|i)</span>로 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고 특정 단어가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률이다.<br>
GloVe의 핵심 아이디어는 임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코러스에서의 동시 등장 확률이 되도록 만드는 것이다.
</div>
```console
> pip install glove_python_binary
```
```python
from glove import Corpus, Glove

corpus = Corpus() 
corpus.fit(result, window=5)
glove = Glove(no_components=100, learning_rate=0.05)

glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)
glove.add_dictionary(corpus.dictionary)

print(glove.most_similar(...))
```
<div class="explain">
<h4>FastText</h4>
FastText는 Word2Vec을 확장시킨 개념이라고 할 수 있다. Word2Vec에서는 단어를 쪼개질 수 없는 단위로 생각하지만 FastText에서는 하나의 단어 안에도 여러 단어들 존재하는 것으로 간주한다. 내부 단어(subword)를 고려하여 학습한다. FastText에서는 각 단어는 글자 단위의 n-gram의 구성으로 취급한다. 이의 장점은 내부 단어를 통해 모르는 단어에 대해서도 다른 단어와의 유사도를 계산할 수 있다. 또한, 등장 빈도가 적은 단어에 대해서도 임베딩의 정확도를 올릴 수 있다. FastText 인공 신경망을 학습한 후에는 데이터셋의 모든 단어의 각 n-gram에 대하여 워드 임베딩이 된다.  
<h4>Keras 임베딩 층</h4>
</div>
```python
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Flatten

sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']
y_train = [1, 0, 0, 1, 1, 0, 1]
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences)
vocab_size = len(tokenizer.word_index) + 1

X_encoded = tokenizer.texts_to_sequences(sentences)
max_len = max(len(l) for l in X_encoded)

X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')
y_train = np.array(y_train)

embedding_dim = 4

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
model.fit(X_train, y_train, epochs=100, verbose=2)
```

<div class="explain">
<h4>ELMo(Embeddings for Language Model)</h4>
기존의 워드 임베딩 방법은 동음이의어처럼 문맥에 따라 달라지는 의미를 반영하지 못한다는 단점이 있었다. 따라서 워드 임베딩 시 문맥을 고려해서 임베딩을 하겠다는 아이디어가 문맥을 반영한 워드 임베딩이다. RNN 언어 모델은 문장으로부터 단어 단위로 입력을 받는데, RNN 내부의 은닉 상태는 시점(time step)이 지날수록 점점 업데이트 된다. 이는 은닉 산태의 값이 문장의 문맥 정보를 점차적으로 반영한다고 볼 수 있다. ELMo는 순방향 RNN 뿐만 아니라 반대 방향으로 문장을 스캔하는 역방향 RNN 또한 활용한다. 이처럼 양쪽 방향의 언어 모델을 둘 다 학습하여 활용한다고 하여 biLM(Bidirectional Language Model)이라고도 한다. biLM은 기본적으로 다층 구조(은닉층이 2개 이상)를 전제로 한다. biLM의 각 시점에 입력이 되는 단어 벡터는 임베딩 층(embedding layer)을 사용해서 얻은 것이 아니라 합성곱 신경망을 이용한 문자 임베딩(character embedding)을 통해 얻은 단어 벡터이다.
<h4>임베딩 시각화</h4>
구글 임베딩 프로젝터: <a href=" https://projector.tensorflow.org/"> https://projector.tensorflow.org/</a>
<h4>자연어 처리를 위한 1D CNN</h4>
1D CNN은 전체 시퀀스 입력 안의 더 작은 시퀀스에 집중하여 정보를 얻어내는 동작을 하는 알고리즘이다.
</div>
```python
from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D

model = Sequential()
model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))
model.add(GlobalMaxPooling1D())
```
<div class="explain">
<h4>문자 임베딩</h4>
문자 임베딩을 워드 임베딩의 대체재로서 쓰거나, 문자 임베딩을 워드 임베딩과 연결(concatenate)하여 신경망의 입력으로 사용하기도 한다.
<ul>
<li> <b>1D CNN</b> <br>
FastText가 문자의 n-gram의 조합을 이용하여 모르는 단어 문제를 해결했듯이 1D DNN을 문자 임베딩에 사용할 경우에는 문자의 n-gram으로부터 정보를 얻어재게 된다. 1D CNN에서 입력으로 단어를 문자 단위로 쪼개서 준다는 것만 다르다. 
</li>
<li><b>BiLSTM</b>
1D CNN과 마찬가지로 기본적으로 단어를 문자 단위로 쪼갠 후 임베딩 층을 사용하여 문자 임베딩을 입력으로 사용한다. 우선, 단어를 문자 단위로 분리하고 임베딩 층을 이용하여 문자를 임베딩한다. 순방향 LSTM으로 단어 순방향으로 문자 임베딩 벡터를 읽고 역방향 LSTM으로 단어의 역방향으로 문자 임베딩 벡터를 읽는다. 그 후 순방향 LSTM의 마지막 시점의 은닉층과 역방향 LSTM의 첫번째 시점의 은닉 상태를 연결(concatenate)한다. 
</li>
</ul>
</div>
<h3>태깅 작업</h3>
<div class="class">
단어 태깅 작업의 대표적인 예시는 단어의 유형이 어느 것인지 알아내는 개체명인식(Named Entity Recognition)과 각 단어의 품사를 알아내는 품사 태깅(Part-of-Speech Tagging)이 있다.<br>
RNN의 다대다(Many to Many) 구조는 RNN의 은닉층의 모든 시점에 대해서 은닉 상태의 값을 출력할 수도, 마지막 시점에 대해서만 은닉 상태의 값을 출력할 수도 있게 지원한다. <br>
개체명 인식기의 성능을 올리기 위한 방법으로 문자 임베딩을 워드 임베딩과 함께 입력으로 사용하는 방법이 있다.
</div>
```python
import nltk
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding
from tensorflow.keras.optimizers import Adam

tagged_sentences = nltk.corpus.treebank.tagged_sents()

sentences, pos_tags = [], [] 
for tagged_sentence in tagged_sentences:
    sentence, tag_info = zip(*tagged_sentence)
    sentences.append(list(sentence)) 
    pos_tags.append(list(tag_info)) 

def tokenize(samples):
  tokenizer = Tokenizer()
  tokenizer.fit_on_texts(samples)
  return tokenizer

src_tokenizer = tokenize(sentences)
tar_tokenizer = tokenize(pos_tags)

vocab_size = len(src_tokenizer.word_index) + 1
tag_size = len(tar_tokenizer.word_index) + 1

X_train = src_tokenizer.texts_to_sequences(sentences)
y_train = tar_tokenizer.texts_to_sequences(pos_tags)

max_len = 150
X_train = pad_sequences(X_train, padding='post', maxlen=max_len)
y_train = pad_sequences(y_train, padding='post', maxlen=max_len)

X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.2, random_state=777)

embedding_dim = 128
hidden_units = 128

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, mask_zero=True))
model.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))
model.add(TimeDistributed(Dense(tag_size, activation=('softmax'))))

model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])
model.fit(X_train, y_train, batch_size=128, epochs=7, validation_data=(X_test, y_test))

index_to_word = src_tokenizer.index_word
index_to_tag = tar_tokenizer.index_word

i = 10 # 확인하고 싶은 테스트용 샘플의 인덱스.
y_predicted = model.predict(np.array([X_test[i]]))
y_predicted = np.argmax(y_predicted, axis=-1)

print("{:15}|{:5}|{}".format("단어", "실제값", "예측값"))
print(35 * "-")

for word, tag, pred in zip(X_test[i], y_test[i], y_predicted[0]):
    if word != 0: # PAD값은 제외함.
        print("{:17}: {:7} {}".format(index_to_word[word], index_to_tag[tag].upper(), index_to_tag[pred].upper()))
```
<div class="explain">
NLTK에서 개체명 인식기(NER chunker)를 지원하고 있다. ne_chunk()는 품사 태깅(pos_tag)이 전제되어야 한다.
</div>
```python
from nltk import word_tokenize, pos_tag, ne_chunk

sentence = "James is working at Disney in London"
tokenized_sentence = pos_tag(word_tokenize(sentence))
ner_sentence = ne_chunk(tokenized_sentence)
print(ner_sentence)
```
<div class="explain">
개체명 인식에서 코퍼스로부터 개체명을 인식하기 위한 방법 중 가장 보편적인 방법은 BIO 태깅으로 B(Begin)은 개체명이 시작되는 부분, I(Inside)는 개체명의 내부 부분, O(Outside)는 개체명이 아닌 부분을 의미한다.
</div>
```python
import re
import numpy as np
import matplotlib.pyplot as plt
import urllib.request
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed
from tensorflow.keras.optimizers import Adam

urllib.request.urlretrieve("https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/12.%20RNN%20Sequence%20Labeling/dataset/train.txt", filename="train.txt")

f = open('train.txt', 'r')
tagged_sentences = []
sentence = []

for line in f:
    if len(line)==0 or line.startswith('-DOCSTART') or line[0]=="\n":
        if len(sentence) > 0:
            tagged_sentences.append(sentence)
            sentence = []
        continue
    splits = line.split(' ') 
    splits[-1] = re.sub(r'\n', '', splits[-1]) 
    word = splits[0].lower() 
    sentence.append([word, splits[-1]]) 

sentences, ner_tags = [], [] 
for tagged_sentence in tagged_sentences:
    sentence, tag_info = zip(*tagged_sentence) 
    sentences.append(list(sentence)) 
    ner_tags.append(list(tag_info))

vocab_size = 4000
src_tokenizer = Tokenizer(num_words=vocab_size, oov_token='OOV')
src_tokenizer.fit_on_texts(sentences)

tar_tokenizer = Tokenizer()
tar_tokenizer.fit_on_texts(ner_tags)

X_train = src_tokenizer.texts_to_sequences(sentences)
y_train = tar_tokenizer.texts_to_sequences(ner_tags)

max_len = 70
X_train = pad_sequences(X_train, padding='post', maxlen=max_len)
y_train = pad_sequences(y_train, padding='post', maxlen=max_len)

X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.2, random_state=777)

y_train = to_categorical(y_train, num_classes=tag_size)
y_test = to_categorical(y_test, num_classes=tag_size)

embedding_dim = 128
hidden_units = 128

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len, mask_zero=True))
model.add(Bidirectional(LSTM(hidden_units, return_sequences=True)))
model.add(TimeDistributed(Dense(tag_size, activation='softmax')))

model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])
model.fit(X_train, y_train, batch_size=128, epochs=8, validation_data=(X_test, y_test))

i = 10

y_predicted = model.predict(np.array([X_test[i]]))

y_predicted = np.argmax(y_predicted, axis=-1)

labels = np.argmax(y_test[i], -1)

print("{:15}|{:5}|{}".format("단어", "실제값", "예측값"))
print(35 * "-")

for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):
    if word != 0:
        print("{:17}: {:7} {}".format(index_to_word[word], index_to_ner[tag].upper(), index_to_ner[pred].upper()))
```
<div class="explain">
정밀도(precision)은 특정 개체라고 예측한 경우 중에서 실제 특정 개체로 판명되어 예측이 일치한 비율로 <span class="math">TP/(TP+FP)</span>이고 재현률(recall)은 전체 특정 개체 중에서 실제 특정 개체하고 정답을 맞춘 비율로 <span class="math">TP/(TP+FN)</span>이다. f1-score은 정밀도와 재현률로부터 조화 평균(harmonic mean)을 구한 것으로 <span class="math">2x(정밀도x재현률)/(정밀도+재현률)</span>이다.
</div>
```python
from seqeval.metrics import classification_report
print(classification_report([labels], [predicted]))
```
<div class="explain">
기존에 CRF 층이 존재하지 않았던 양방향 LSTM 모델은 활성화 함수를 지난 시점에서 개체명을 결정했지만, CRF 층을 추가한 모델에서는 활성화 함수의 결과들이 CRF 층의 입력으로 전달한다.
</div>
```console
> pip install keras-crf
```
```python
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, LSTM, Input, Bidirectional, TimeDistributed, Embedding, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from keras_crf import CRFModel
from seqeval.metrics import f1_score, classification_report

embedding_dim = 128
hidden_units = 64
dropout_ratio = 0.3

sequence_input = Input(shape=(max_len,),dtype=tf.int32, name='sequence_input')

model_embedding = Embedding(input_dim=vocab_size,
                            output_dim=embedding_dim,
                            input_length=max_len)(sequence_input)

model_bilstm = Bidirectional(LSTM(units=hidden_units, return_sequences=True))(model_embedding)

model_dropout = TimeDistributed(Dropout(dropout_ratio))(model_bilstm)

model_dense = TimeDistributed(Dense(tag_size, activation='relu'))(model_dropout)

base = Model(inputs=sequence_input, outputs=model_dense)
model = CRFModel(base, tag_size)
model.compile(optimizer=tf.keras.optimizers.Adam(0.001), metrics='accuracy')

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('bilstm_crf/cp.ckpt', monitor='val_decode_sequence_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=True)

history = model.fit(X_train, y_train_int, batch_size=128, epochs=15, validation_split=0.1, callbacks=[mc, es])

model.load_weights('bilstm_crf/cp.ckpt')

i = 13 # 확인하고 싶은 테스트용 샘플의 인덱스.
y_predicted = model.predict(np.array([X_test[i]]))[0] 
labels = np.argmax(y_test[i], -1) 

print("{:15}|{:5}|{}".format("단어", "실제값", "예측값"))
print(35 * "-")

for word, tag, pred in zip(X_test[i], labels, y_predicted[0]):
    if word != 0: # PAD값은 제외함.
        print("{:17}: {:7} {}".format(index_to_word[word], index_to_ner[tag], index_to_ner[pred]))

def sequences_to_tag_for_crf(sequences): 
    result = []
    for sequence in sequences: 
        word_sequence = []
        for pred_index in sequence:
            word_sequence.append(index_to_ner[pred_index].replace("PAD", "O"))
        result.append(word_sequence)
    return result

pred_tags = sequences_to_tag_for_crf(y_predicted)
test_tags = sequences_to_tag(y_test)

print("F1-score: {:.1%}".format(f1_score(test_tags, pred_tags)))
print(classification_report(test_tags, pred_tags))
```
<h3>출처</h3>
<a href="https://wikidocs.net/book/2155">딥러닝을 이용한 자연어 처리 입문</a>
