---
layout: post
title: "빅데이터 혁신 융합대학 인턴 LLM 개발- NLP 기본 개념"
date: 2023-12-19 09:00:23 +0900
categories:  ["NLP","Intern"]
---
# NLP 기본 개념
<h3>자연어 처리를 위한 패키지</h3>
<ul>
<li>NLTK<br>
<div class="explain">
자연어 처리를 위한 파이썬 패키지로 자연어 처리를 위한 Tokenizer 등과 코퍼스를 다운로드 할 수 있다. <br>
자연어 처리를 위한 여러 패키지들을 다운받기 위해서는 nltk.download() 코드를 실행하면 된다.
</div>
</li>
</ul>
```console
> pip install nltk
```
```python
import nltk
nltk.download()
```
<ul>
<li>KoNLPy<br>
<div class="explain">
한국어 자연어 처리를 위한 현태소 분석기 패키지이다. <br>
KoNLPy는 JAVA로 개발되었기 때문에 JAVA와 Python을 연결해주는 JPype를 설치해야한다. 또한, JDK1.7 이상의 버전이 설치되어 있어야한다.<br>
JPype는 https://github.com/jpype-project/jpype/releases에서 다운받을 수 있으며 파이썬 버전과 OS에 맞게 다운 받아야한다.
</div>
</li>
</ul>
```console
> pip install konlpy
```
```console
> pip install JPype1-0.6.3-cp36-cp36m-win_amd64.whl
```
<ol>
<li>Tokenization<br>
<div class="explain">토큰화(Tokenization)은 코퍼스(텍스트 데이터)를 토큰 단위로 나누는 작업을 의미한다. 토큰의 단위는 단어, 단어구, 문장 등 다양할 수 있다.
</div>
</li>
</ol>
```python
from nltk.tokenize import word_tokenize
from nltk.tokenize import WordPunctTokenizer
from tensorflow.keras.preprocessing.text import text_to_word_sequence
from nltk.tokenize import TreebankWordTokenizer
print(word_tokenize(...))
print(WordPunctTokenizer().tokenize(...))
print(text_to_word_sequence(...))
tokenizer = TreebankWordTokenizer()
print(tokenizer.tokenize(...))
```
<div class="explain">
단어 단위로 토큰화 하기 위해서는 NLTK의 word_tokenize, WordPunctTokenizer나 Keras의 text_to_word_sequence를 사용할 수 있다.<br>
word_tokenize와 WordPunctTokenizer는 apostrophe를 다르게 처리한다. word_tokenize는 Do not의 축약인 Don't를 Do와 n't로 John's를 John과 's로 분리하지만 WordPunctTokenizer는 구두점을 기준으로 분리하기 때문에 Don't는 Don과 '와 t로 John's는 John과 '와 s로 분리한다.<br>
Keras의 text_to_word_sequence는 기본적으로 소문자로 모두 바꾸고 구두점을 제거하지만 don't나 John's와 같은 경우에는 apostrophe를 보존한다.<br>
표준으로 사용되고 있는 Penn Treebank Tokenization은 하이픈으로 구성된 단어는 분리하지 않고 apostrophe가 접어와 함께하는 경우에는 단어를 분리한다.<br>
</div>
```python
from nltk.tokenize import sent_tokenize
print(sent_tokenize(...))
```
```console
> pip install kss
```
```python
import kss
print(kss.split_sentences(...))
```
<div class="explain">
문장을 기준으로 토큰화를 하기 위해서는 NLTK의 sent_tokenize를 사용할 수 있다. 한국어 문장은 KSS 패키지를 추가적으로 설치하고 토큰화할 수 있다.
</div>
```python
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

tokenized_sentence = word_tokenize(...)
print(pos_tag(tokenized_sentence))
```
```python
from konlpy.tag import Okt
from konlpy.tag import Kkma

okt = Okt()
kkma = Kkma()

print(okt.morphs(...))
print(okt.pos(...))
print(okt.nouns(...))
print(kkma.morphs(...))
print(kkma.pos(...))
print(kkma.nouns(...))
```
<div class="explain">
NLTK에서는 Penn Treebank POS Tags 기준을 사용하여 품사를 매핑한다.<br>
Penn Treebank POG Tags에서 PRP는 인칭 대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사, NNP는 고유 명사, NNS는 복수형 명사, CC는 접속사, DT는 관사를 의미한다.<br>
한국어에서 형태소를 토큰화하기 위해서는 KoNLPy의 Okt, Mecab, Komoran, Hannanum,Kkma등을 사용할 수 있다.<br>
morphs는 형태소를 추출하고 pos는 품사를 매핑하고 nouns는 명사를 추출한다.
</div>
<ol start="2">
<li>Cleaning&Normalization<br>
<div class="explain">정제(cleaning)은 코퍼스로부터 노이즈 데이터를 제거하고 정규화(normalization)은 표현 방법이 다른 단어들을 통합하여 같은 단어로 만든다.<br>
정규화를 위해서는 대문자나 소문자 통합, 어간 추출(stemming)과 표제어 추출(lemmatization)을 수행할 수 있다. <br>
정제를 위해서는 불필요한 단어일 가능성이 큰 등장 빈도가 적은 단어나 길이가 짧은 단어 등을 제거한다. <br>이를 위해서 정규 표현식(Regular Expression)이 유용하다.<br>
표제어(Lemma)는 기본 사전형 단어로 am, are, is는 서로 다른 스펠링이지만 동일하게 기본형이 be이다. 어간(stem)은 단어의 의미를 담고 있는 단어의 핵심 부분이고 접사(affix)는 단어에 추가적인 의미를 주는 부분이다.<br>
어간을 추출하기 위해서는 정해진 규칙만 보고 어미를 자른다. 이는 단순한 작업이기 때문에 어간으로 추출된 단어가 사전에 존재하지 않는 단어일 수 있다.
</div>
</li>
</ol>
```python
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
print(lemmatizer.lemmatize(...))
```
<div class="explain">NLTK는 표제어 추출을 위해 WordNetLemmatizer를 제공한다.</div>

```python
from nltk.stem import PorterStemmer
from nltk.stem import LancasterStemmer

porter_stemmer = PorterStemmer()
lancaster_stemmer = LancasterStemmer()

print(porter_stemmer.stem(...))
print(lancaster_stemmer.stem(...))
```
<div class="explain">NLTK에서는 포터 알고리즘과 랭커스터 스태머 알고리즘을 지원한다. </div>
```python
from nltk.corpus import stopwords

stop_words_list = stopwords.words('english')
result = []
if word not in stop_words: 
        result.append(word) 
```
<div class="explain">stopwords.words("english")는 NLTK가 정의한 영어 불용어 리스트를 가져온다. 이 리스트에는 100개 이상의 영어 단어들을 불용어로 가지고 있다. </div>
<div class="explain"> 파이썬에서 정규 표현식을 사용하기 위해서는 re 모듈을 사용한다.<br>
정규 표현식에서 나타나는 특수 문자들의 의미는 다음과 같다.
</div>
<table style="border:1px solid gray; text-align:center; border-collapse:collapse;">
<tr >
<td style="border:1px solid gray;">.</td>
<td style="border:1px solid gray;">한 개의 임의의 문자를 의미한다.</td>
</tr>
<tr >
<td style="border:1px solid gray;">?</td>
<td style="border:1px solid gray;">0이나 1개가 존재</td>
</tr>
<tr >
<td style="border:1px solid gray;">*</td>
<td style="border:1px solid gray;">0개 이상 존재</td>
</tr>
<tr >
<td style="border:1px solid gray;">+</td>
<td style="border:1px solid gray;">1개 이상 존재</td>
</tr>
<tr >
<td style="border:1px solid gray;">^</td>
<td style="border:1px solid gray;">뒤의 문자열로 문자열이 시작</td>
</tr>
<tr >
<td style="border:1px solid gray;">$</td>
<td style="border:1px solid gray;">앞의 문자열로 문자열이 끝남</td>
</tr>
<tr >
<td style="border:1px solid gray;">{숫자}</td>
<td style="border:1px solid gray;">숫자만큼 반복</td>
</tr>
<tr >
<td style="border:1px solid gray;">{숫자1,숫자2}</td>
<td style="border:1px solid gray;">숫자1 이상 숫자2 이하만큼 반복</td>
</tr>
<tr >
<td style="border:1px solid gray;">{숫자1,}</td>
<td style="border:1px solid gray;">숫자 이상만큼 반복</td>
</tr>
<tr >
<td style="border:1px solid gray;">[]</td>
<td style="border:1px solid gray;">대괄호 안의 문자 조건 중 하나와 매칭,[a-zA-Z]로 알파벳 전체를 문자 조건으로 줄 수 있다.</td>
</tr>
<tr >
<td style="border:1px solid gray;">[^문자]</td>
<td style="border:1px solid gray;">해당 문자를 제외한 문자를 매치</td>
</tr>
<tr >
<td style="border:1px solid gray;">|</td>
<td style="border:1px solid gray;">또는의 의미를 갖는다. A|B는 A 또는 B를 의미한다.</td>
</tr>
</table>
```python
import re
r = re.compile("ab.")
print(r.match("kkkabc"))
r.search("kkkabc") 
print(re.split(" ", text))
print(re.findall("ab.", text))
print(re.sub('[^a-zA-Z]', ' ', text))
```
<div class="explain">
search는 모듈 함수에 문자열을 넣어 매치되는지 확인하지만 match는 문자열의 시작부분부터 매치한다.
<br>
split은 정규 표현식을 기준으로 문자열을 분리한다.<br>
findall은 매칭되는 모든 문자열을 반환한다.<br>
sub는 문자열을 대체한다.
</div>
<ol start="3">
<li>Encoding<br>
<div class="explain">컴퓨터는 텍스트보다는 숫자를 잘 처리한다. 따라서 텍스트를 숫자로 인코딩하는 과정이 필요하다.<br>
텍스트를 숫자로 인코딩하기 위해서는 단어의 등장 빈도에 따라 정렬하여 순서대로 인덱스를 부여하는 방법, One-Hot 인코딩 등 다양한 방법을 사용할 수 있다.
</div>
</li>
</ol>
```python
from nltk import FreqDist
import numpy as np
vocab = FreqDist(np.hstack(preprocessed_sentences))
vocab_size = 5
vocab = vocab.most_common(vocab_size)
word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}
```
```python
from tensorflow.keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(preprocessed_sentences) #빈도 수에 따라 인덱스가 주어진다.
print(tokenizer.word_index) #각 단어에 인덱스가 어떻게 부여되었는지를 보려면, word_index를 사용
print(tokenizer.texts_to_sequences(preprocessed_sentences)) #입력으로 들어온 코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환
```
```python
from tensorflow.keras.preprocessing.sequence import pad_sequences
padded = pad_sequences(encoded) #앞에 0을 채움
padded = pad_sequences(encoded, padding='post') #뒤에 0을 채움
```
<div class="explain"> 문장의 길이가 다르면 0으로 패딩을 줘서 길이를 맞춘다.
</div>
```python
from tensorflow.keras.utils import to_categorical
one_hot = to_categorical(encoded)
print(one_hot)
```
<div class="explain"> 원-핫 인코딩은 단어의 개수가 늘어날 수록 벡터를 저장하기 위한 공간이 늘어난다는 단점이 있다.또한, 단어의 유사도를 표현하지 못한다. 이러한 단점을 해결하기 위해서는 카운팅 기반의 LSA(잠재 의미 분석), HAL과 예측 기반으로 벡터화하는 NNLM,RNNLM,Word2Vec, FastText 등을 사용할 수 있다.이 두 방법을 모두 사용하는 Glove라는 방법도 존재한다. </div>
<h3>출처</h3>
<a href="https://wikidocs.net/book/2155">딥러닝을 이용한 자연어 처리 입문</a>
