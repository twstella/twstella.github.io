---
layout: post
title: "빅데이터 혁신 융합대학 인턴 LLM 개발- NLP 심화 개념(2)"
date: 2024-01-02 09:00:23 +0900
categories:  ["NLP","Intern"]
---
# NLP 심화(2)
<h3>어텐션 메커니즘</h3>
<div class="explain">
 seq2seq은 인코더에서 입력 시퀀스를 컨텍스트 벡터라는 하나의 고정된 크기의 벡터 표현으로 압축하고 디코더는 이 컨텍스트 벡터를 통해서 출력 시퀀스를 만들낸다. 그러나 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하니까 정보 손실이 발생하고 RNN의 고질적인 문제인 기울기 문제가 있다. 어텐션의 기본적인 아이디어는 디코더에서 출력 단어를 예측하는 매 시점마다 인코더에서 전체 입력 문장을 다시 한 번 참고한다. 이때 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중해서 본다. 
 어텐션 함수는 주어진 Query에 대하여 Key와의 유사도를 각각 구하고 이 구해진 유사도를 키와 매핑되어 있는 각각의 Value에 반영한다. 그리고 유사도가 반영된 Value를 모두 더하여 반환해준다. 여기서 이 값을 어텐션 값(Attention Value)라고 한다. 다양한 종류의 어탠션들은 중간 수식인 스코어 함수에 따라 달라진다. 어탠션 스코어를 구하는 방법은 <span class="math">dot, scaled dot, general, concat, location-base</span> 등이 있으며 concat를 쓰는 어탠션은 Bahdanau 어탠션이라고도 부른다.
 <h4>Dot-Product Attention</h4>
 디코더에서 출력 단어를 예측하기 위해서 인코더에서 각 입력 단어가 각각이 출력 단어를 예측할 때 얼마나 도움이 되는 지 수치화하고 이를 디코더에 전달한다. 
 <ol>
 <li><b>Attention Score를 구한다.</b><br>
어텐션 스코어(Attention Score)는 현재 디코더의 시점에서 단어를 예측하기 위해 인코더의 모든 은닉 상태 각각이 디코더의 현 시점의 은닉 상태 <span class="math">S<sub>t</sub></span>와 얼마나 유사한지 판단하는 스코어 값이다. Dot-Product Attention에서는 이 스코어 값을 계산하기 위해 디코더의 현 은닉 상태를 전치(Transpose)하고 각 은닉 상태의 내적을 수행한다. <span class="math">S<sub>t</sub></span>와 인코더의 모든 은닉 상태의 어텐션 스코어의 모음값을 <span class="math">e<sup>t</sup></span>는 <span class="math">[score(S<sub>t</sub>,h<sub>1</sub>),score(S<sub>t</sub>,h<sub>2</sub>),...]</span>이다. h<sub>i</sub>는 인코더의 은닉 상태 각각이다.</li>
<li><b>Softmax 함수를 통해 Attention Distribution을 구한다.</b><br>
  <span class="math">e<sup>t</sup></span>에 소프트맥스 함수를 적용하여 모든 값을 더하면 1이 되는 확률 분표를 만든다. 이를 어텐션 분포라고 하며 각각의 값을 어텐션 가중치(Attention Weight)라고 한다. 한 디코더 시점애서의 어텐션 분포 <span class="math">&alpha;<sup>t</sup></span>는 <span class="math">softmax(e<sup>t</sup>)</span>이다.
</li>
<li><b>각 인코더의 어텐션 가중치와 은닉 상태를 가중하여 Attention Value를 구한다.</b><br>
  어텐션의 최종 결과값을 얻기 위해서는 각 인코딩의 은닉 상태와 어텐션 가중치값들을 곱하고 최종적으로 모두 더한다. 어텐션 함수의 출력 값인 Attention Value <span class="math">a<sub>t</sub></span>은 <span class="math">&alpha;<sub>1,t</sub>h<sub>1</sub>+&alpha;<sub>2,t</sub>h<sub>2</sub>+...+&alpha;<sub>N,t</sub>h<sub>N</sub></span>이다. 이 어텐션 값 <span class="math">a<sub>t</sub></span>는 문맥을 포함하고 있다고 하여 Context Vector라고도 부르며 seq2seq에서 인코더의 마지막 은닉 상태를 컨텍스트 벡터라고 부른 것과 다르다. 
</li>
<li><b>하이퍼볼릭탄젠트로 출력층 연산의 입력이 되는 새로운 벡터 계산</b><br>
<span class="math">v<sub>t</sub></span>를 가중치 행렬과 곱한 후 하이퍼볼릭탄젠트 함수를 지나도록 하여 출력층 연산을 위한 새로운 벡터<span class="math">s'<sub>t</sub></span>를 계산한다. 
</li>
<li><b><span class="math">s'<sub>t</sub></span>를 출력층의 입력으로 사용한다.</b><br>
<span class="math">y'<sub>t</sub>=Softmax(W<sub>y</sub>s'<sub>t</sub>+b<sub>y</sub>)</span>
</li>
 </ol>
<h4>Bahdanau Attention</h4>
<ol>
<li>
<b>Attention score 구하기</b>
Dot-Product Attention에서 디코더의 t 시점의 은닉 상태를 사용한거와 달리 이 어탠션은 t-1 시점의 은닉 상태를 사용한다. <span class="math">e<sup>t</sup>=score(S<sub>t-1</sub>,H)=W<sub>a,T</sub>Tanh(W<sub>b</sub>S<sub>t-1</sub>+W<sub>c</sub>H)</span>로 구한다.
</li>
<li><b>Softmax로 Attention Distribution 구하기</b>
<span class="math">e<sup>t</sup></span>에 소프트맥스 함수를 적용하여 Attention Distribution을 구한다. 그리고 각각의 값을 Attention Weight라고 한다.
</li>
<li>
<b>각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 Attention Value를 구한다.</b>
각 인코더의 은닉 상태와 어텐션 가중치값들을 곱하고 최종적으로 모두 더한다. 이 결과 벡터는 context vector라고 부른다.
</li>
<li><b>컨텍스트 벡터로부터 <span class="math">S<sub>t</sub></span>를 구한다.</b>
컨텍스트 벡터와 현재 시점의 입력인 단어의 임베딩 벡터를 concate하고 현재 시점의 새로운 입력으로 사용한다. 그리고 이전 시점의 셀로부터 전달 받은 은닉 상태와 현재 시점의 새로운 입력으로부터 은닉 상태를 구한다.
</li>
</ol>
</div>
```python
from tensorflow.keras.datasets import imdb
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout
from tensorflow.keras import Input, Model
from tensorflow.keras import optimizers
import os


vocab_size = 10000
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)
max_len = 500
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

class BahdanauAttention(tf.keras.Model):
  def __init__(self, units):
    super(BahdanauAttention, self).__init__()
    self.W1 = Dense(units)
    self.W2 = Dense(units)
    self.V = Dense(1)

  def call(self, values, query):
    hidden_with_time_axis = tf.expand_dims(query, 1)

    score = self.V(tf.nn.tanh(
        self.W1(values) + self.W2(hidden_with_time_axis)))

    attention_weights = tf.nn.softmax(score, axis=1)

    context_vector = attention_weights * values
    context_vector = tf.reduce_sum(context_vector, axis=1)

    return context_vector, attention_weights

sequence_input = Input(shape=(max_len,), dtype='int32')
embedded_sequences = Embedding(vocab_size, 128, input_length=max_len, mask_zero = True)(sequence_input)

lstm = Bidirectional(LSTM(64, dropout=0.5, return_sequences = True))(embedded_sequences)

lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional \
  (LSTM(64, dropout=0.5, return_sequences=True, return_state=True))(lstm)

state_h = Concatenate()([forward_h, backward_h])
state_c = Concatenate()([forward_c, backward_c])

attention = BahdanauAttention(64)
context_vector, attention_weights = attention(lstm, state_h)

dense1 = Dense(20, activation="relu")(context_vector)
dropout = Dropout(0.5)(dense1)
output = Dense(1, activation="sigmoid")(dropout)
model = Model(inputs=sequence_input, outputs=output)

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs = 3, batch_size = 256, validation_data=(X_test, y_test), verbose=1)

print("\n 테스트 정확도: %.4f" % (model.evaluate(X_test, y_test)[1]))
```
<h3>트랜스포머</h3>
기존의 seq2seq에서 인코더-디코더 구조는 유지하면서 RNN을 사용하지 않고 어텐션만으로 구성한 모델이다. 이전의 seq2seq에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점을 가지는 구조였다면 트랜스포머에서는 인코더와 디코더가 N개의 단위로 구성되는 구조이다. 트랜스포머에서는 Encoder Self-Attention, Masked Decoder Attention, Encoder-Decoder Attention을 사용한다. Self-Attention은 자기 자신에게 어텐션을 수행한다는 의미이다. Self-Attention을 수행하기 위해서는 각각의 단어마다 Q,K,V를 얻는다. 그 후 기존의 어텐션 메커니즘과 같이 각 Q벡터는 모든 K벡터에 대하여 Attention Score을 구하고 Attention Distribution을 구한 뒤에 이를 사용하여 모든 V벡터를 가중합하여 Attention Value 또는 context vector를 구한다. 그리고 이를 모든 Q벡터에 대하여 반복한다. 트랜잭션에서는 scoring fucntion으로 Scaled-dot-product Attention을 사용한다. 인코딩은 두 개의 서브층으로 구성되는데 첫번째 서브층은 Multi-head Attention이다. Multi-head Attention은 어텐션을 병렬로 수행한다. 두번째 서브층인 FFNN은 인코더와 디코더 모두 가지고 있다. 트랜스포머의 디코더에서는 현재 시점의 예측에서 현재 시점보다 미래에 있는 단어들을 참고하지 못하도록 룩-어헤드 마스크(look-ahead mask)를 도입했다. 룩-어헤드 마스크(look-ahead mask)는 디코더의 첫번째 서브층에서 이루어진다. 디코더의 첫번째 서브층인 멀티 헤드 셀프 어텐션 층은 인코더의 첫번째 서브층인 멀티 헤드 셀프 어텐션 층과 동일한 연산을 수행한다. 오직 다른 점은 어텐션 스코어 행렬에서 마스킹을 적용한다는 점만 다르다. 디코더의 두번째 서브층은 멀티 헤드 어텐션을 수행한다는 점에서는 이전의 어텐션들(인코더와 디코더의 첫번째 서브층)과는 공통점이 있으나 이번에는 셀프 어텐션이 아니다. 트랜스포머의 구현 코드는 아래의 출처에 있다.
<h3>출처</h3>
<a href="https://wikidocs.net/book/2155">딥러닝을 이용한 자연어 처리 입문</a>
